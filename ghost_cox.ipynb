{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeVFb5YRwS9m/eQ299pWlU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pygam scikit-survival"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBeYEPuBsn3S",
        "outputId": "214d9a33-a7a4-46e0-ef4f-578ca56ca48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygam in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Collecting scikit-survival\n",
            "  Downloading scikit_survival-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from pygam) (1.26.4)\n",
            "Requirement already satisfied: progressbar2<5.0.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pygam) (4.5.0)\n",
            "Requirement already satisfied: scipy<1.12,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from pygam) (1.11.4)\n",
            "Collecting ecos (from scikit-survival)\n",
            "  Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scikit-survival) (1.4.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from scikit-survival) (2.10.2)\n",
            "Collecting osqp<1.0.0,>=0.6.3 (from scikit-survival)\n",
            "  Downloading osqp-0.6.7.post3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from scikit-survival) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<1.7,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from scikit-survival) (1.6.1)\n",
            "Collecting qdldl (from osqp<1.0.0,>=0.6.3->scikit-survival)\n",
            "  Downloading qdldl-0.1.7.post5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: python-utils>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from progressbar2<5.0.0,>=4.2.0->pygam) (3.9.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.6.1->scikit-survival) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival) (1.17.0)\n",
            "Requirement already satisfied: typing_extensions>3.10.0.2 in /usr/local/lib/python3.11/dist-packages (from python-utils>=3.8.1->progressbar2<5.0.0,>=4.2.0->pygam) (4.13.1)\n",
            "Downloading scikit_survival-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading osqp-0.6.7.post3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.3/298.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdldl-0.1.7.post5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: qdldl, ecos, osqp, scikit-survival\n",
            "  Attempting uninstall: osqp\n",
            "    Found existing installation: osqp 1.0.3\n",
            "    Uninstalling osqp-1.0.3:\n",
            "      Successfully uninstalled osqp-1.0.3\n",
            "Successfully installed ecos-2.0.14 osqp-0.6.7.post3 qdldl-0.1.7.post5 scikit-survival-0.24.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW1sX8ugn86n"
      },
      "outputs": [],
      "source": [
        "# %%writefile ghost_variables.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.exceptions import NotFittedError\n",
        "try:\n",
        "    from pygam import LinearGAM # Using LinearGAM for simplicity\n",
        "except ImportError:\n",
        "    LinearGAM = None # Handle optional dependency\n",
        "    # print(\"Warning: Package 'pygam' not found. 'gam' estimator type will not be available.\") # Less noisy\n",
        "\n",
        "from typing import Type, Dict, Any, List, Union, Optional, Tuple\n",
        "import warnings\n",
        "import joblib # For parallelization\n",
        "import inspect # For robust parameter handling\n",
        "\n",
        "class GhostVariableEstimator:\n",
        "    \"\"\"\n",
        "    Estimates ghost variables E(Z|X_{-j}) for each variable Z_j in a dataset X.\n",
        "    Supports different regression models for estimating the conditional expectation.\n",
        "    Based on the concept proposed by Peña & Delicado (2023).\n",
        "\n",
        "    Args:\n",
        "        estimator_type (str): Model type ('lm', 'rf', 'gam'). Defaults to 'rf'.\n",
        "        n_jobs (Optional[int]): Parallel jobs for estimating different ghosts.\n",
        "                                  Defaults to 1 (serial). Set to -1 to use all cores,\n",
        "                                  but be aware of potential serialization errors.\n",
        "        estimator_params: Keyword arguments passed to the underlying regressor.\n",
        "    \"\"\"\n",
        "    SUPPORTED_ESTIMATORS: Dict[str, Optional[Type[BaseEstimator]]] = {\n",
        "        'lm': LinearRegression,\n",
        "        'rf': RandomForestRegressor,\n",
        "        'gam': LinearGAM\n",
        "    }\n",
        "\n",
        "    DEFAULT_PARAMS: Dict[str, Dict[str, Any]] = {\n",
        "        'lm': {'n_jobs': None}, # LM's n_jobs is different, let joblib handle outer parallelism\n",
        "        'rf': {'n_estimators': 100, 'random_state': 42, 'n_jobs': 1, 'max_depth': 10, 'min_samples_leaf': 5},\n",
        "        'gam': {'n_splines': 10, 'max_iter': 100, 'tol': 1e-3}\n",
        "    }\n",
        "\n",
        "    # ***** CHANGE: Default n_jobs to 1 *****\n",
        "    def __init__(self, estimator_type: str = 'rf', n_jobs: Optional[int] = 1, **estimator_params):\n",
        "        estimator_key = estimator_type.lower()\n",
        "        if estimator_key not in self.SUPPORTED_ESTIMATORS:\n",
        "            raise ValueError(f\"Invalid estimator_type '{estimator_type}'. Choose from {list(k for k,v in self.SUPPORTED_ESTIMATORS.items() if v is not None)}.\")\n",
        "        if self.SUPPORTED_ESTIMATORS[estimator_key] is None:\n",
        "             # Added print here for clarity if user tries 'gam' without pygam\n",
        "             print(f\"Optional dependency for estimator type '{estimator_key}' not found. Please install it (e.g., pip install pygam).\")\n",
        "             raise ImportError(f\"Estimator type '{estimator_key}' requires an optional dependency that is not installed.\")\n",
        "\n",
        "        self.estimator_key: str = estimator_key\n",
        "        self.estimator_class: Type[BaseEstimator] = self.SUPPORTED_ESTIMATORS[estimator_key]\n",
        "\n",
        "        merged_params = self.DEFAULT_PARAMS.get(estimator_key, {}).copy()\n",
        "        merged_params.update(estimator_params)\n",
        "        self.estimator_params: Dict[str, Any] = merged_params\n",
        "        self.n_jobs = n_jobs # Store user preference\n",
        "        self._fitted_estimators: Dict[str, BaseEstimator] = {}\n",
        "\n",
        "        print(f\"Initialized GhostVariableEstimator with {self.estimator_key.upper()} (params: {self.estimator_params}, requested n_jobs={self.n_jobs})\")\n",
        "        if self.n_jobs != 1:\n",
        "            print(\"Note: Using n_jobs != 1 may cause serialization errors depending on the environment/estimators.\")\n",
        "\n",
        "    def _get_estimator_instance(self) -> BaseEstimator:\n",
        "        \"\"\"Instantiates a new underlying estimator model with configured parameters.\"\"\"\n",
        "        sig = inspect.signature(self.estimator_class.__init__)\n",
        "        valid_keys = {p.name for p in sig.parameters.values() if p.kind == p.POSITIONAL_OR_KEYWORD or p.kind == p.KEYWORD_ONLY}\n",
        "        if 'self' not in valid_keys and any(p.name == 'self' for p in sig.parameters.values()):\n",
        "             valid_keys.add('self')\n",
        "        init_params = {k: v for k, v in self.estimator_params.items() if k in valid_keys}\n",
        "        try:\n",
        "             return self.estimator_class(**init_params)\n",
        "        except TypeError as e:\n",
        "             warnings.warn(f\"Error initializing {self.estimator_key.upper()} with filtered params {init_params}: {e}. Trying default.\", UserWarning)\n",
        "             try:\n",
        "                 return self.estimator_class()\n",
        "             except Exception as e_def:\n",
        "                  raise RuntimeError(f\"Failed to initialize {self.estimator_key.upper()} with both provided and default params.\") from e_def\n",
        "\n",
        "\n",
        "    def _estimate_single_ghost_job(self, X_df: pd.DataFrame, target_var_name: str) -> Optional[pd.Series]:\n",
        "        \"\"\"\n",
        "        Internal helper: Estimates E(Z|X_{-j}) for a single target variable Z_j.\n",
        "        Designed for parallel execution. Returns None on failure.\n",
        "        \"\"\"\n",
        "        current_job_id = f\"{target_var_name} (using {self.estimator_key})\"\n",
        "        try:\n",
        "            if target_var_name not in X_df.columns:\n",
        "                 warnings.warn(f\"[{current_job_id}] Target variable not found in DataFrame.\", UserWarning)\n",
        "                 return None\n",
        "\n",
        "            if X_df.shape[1] <= 1:\n",
        "                warnings.warn(f\"[{current_job_id}] Cannot estimate ghost with <= 1 feature. Returning original.\", UserWarning)\n",
        "                return X_df[target_var_name].astype(float)\n",
        "\n",
        "            target_series = X_df[target_var_name]\n",
        "            feature_df = X_df.drop(columns=[target_var_name])\n",
        "\n",
        "            if not pd.api.types.is_numeric_dtype(target_series):\n",
        "                 warnings.warn(f\"[{current_job_id}] Target is not numeric. Cannot estimate ghost. Returning original.\", UserWarning)\n",
        "                 return target_series.astype(float) # Attempt conversion\n",
        "\n",
        "            # Feature preprocessing\n",
        "            if self.estimator_key in ['lm', 'rf']:\n",
        "                numeric_features = feature_df.select_dtypes(include=np.number)\n",
        "                if numeric_features.empty:\n",
        "                     warnings.warn(f\"[{current_job_id}] No numeric features found for {self.estimator_key}. Returning original.\", UserWarning)\n",
        "                     return target_series.astype(float)\n",
        "                if numeric_features.shape[1] < feature_df.shape[1]:\n",
        "                     warnings.warn(f\"[{current_job_id}] Non-numeric features dropped for {self.estimator_key}.\", UserWarning)\n",
        "                feature_df_processed = numeric_features\n",
        "            elif self.estimator_key == 'gam':\n",
        "                 numeric_features = feature_df.select_dtypes(include=np.number)\n",
        "                 if numeric_features.empty:\n",
        "                     warnings.warn(f\"[{current_job_id}] No numeric features found for GAM. Returning original.\", UserWarning)\n",
        "                     return target_series.astype(float)\n",
        "                 if numeric_features.shape[1] < feature_df.shape[1]:\n",
        "                     warnings.warn(f\"[{current_job_id}] Non-numeric features dropped for GAM (requires encoding).\", UserWarning)\n",
        "                 feature_df_processed = numeric_features\n",
        "            else:\n",
        "                 feature_df_processed = feature_df # Should not happen\n",
        "\n",
        "            # Basic Imputation (example, consider better strategies)\n",
        "            if feature_df_processed.isnull().any().any():\n",
        "                 warnings.warn(f\"[{current_job_id}] Features contain NaNs after selection. Applying median imputation.\", UserWarning)\n",
        "                 for col in feature_df_processed.columns:\n",
        "                     if feature_df_processed[col].isnull().any():\n",
        "                         median_val = feature_df_processed[col].median()\n",
        "                         # Use .loc to avoid SettingWithCopyWarning\n",
        "                         feature_df_processed.loc[:, col] = feature_df_processed[col].fillna(median_val)\n",
        "\n",
        "\n",
        "            estimator = self._get_estimator_instance()\n",
        "            estimator.fit(feature_df_processed.values, target_series.values)\n",
        "            Z_ghost = estimator.predict(feature_df_processed.values)\n",
        "            # self._fitted_estimators[target_var_name] = estimator # Optional storage\n",
        "\n",
        "            return pd.Series(Z_ghost.astype(float), index=X_df.index, name=target_var_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"[{current_job_id}] Error during estimation: {e}. Returning None.\", UserWarning)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def estimate_all_ghosts(self, X_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Estimates ghost variables E(Z|X_{-j}) for all columns Z_j in the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            X_data (pd.DataFrame): Input data (features). Must be pandas DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame containing estimated ghost variables for successfully\n",
        "                          processed columns. Columns where estimation failed are omitted.\n",
        "        \"\"\"\n",
        "        if not isinstance(X_data, pd.DataFrame) or X_data.empty:\n",
        "            raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n",
        "        if X_data.isnull().any().any():\n",
        "             warnings.warn(\"Input data contains NaNs. Consider imputation before calling estimate_all_ghosts.\", UserWarning)\n",
        "\n",
        "        print(f\"\\nEstimating all ghost variables using {self.estimator_key.upper()}...\")\n",
        "        target_vars = list(X_data.columns) # Keep original order\n",
        "\n",
        "        # Use joblib for parallelism, but respect n_jobs=1 for serial execution\n",
        "        # Set verbosity based on n_jobs; less verbose for serial run.\n",
        "        verbosity = 5 if self.n_jobs != 1 else 0\n",
        "        try:\n",
        "            with joblib.Parallel(n_jobs=self.n_jobs, backend='loky', verbose=verbosity) as parallel:\n",
        "                 results = parallel(\n",
        "                     joblib.delayed(self._estimate_single_ghost_job)(X_data.copy(), var_name)\n",
        "                     for var_name in target_vars\n",
        "                 )\n",
        "        except Exception as e:\n",
        "             # Catch potential top-level joblib errors (like the BrokenProcessPool)\n",
        "             warnings.warn(f\"Joblib parallel execution failed: {e}. Attempting serial execution.\", RuntimeWarning)\n",
        "             results = [self._estimate_single_ghost_job(X_data.copy(), var_name) for var_name in target_vars]\n",
        "\n",
        "\n",
        "        # Process results\n",
        "        valid_results_map = {res.name: res for res in results if res is not None}\n",
        "\n",
        "        if not valid_results_map:\n",
        "             raise RuntimeError(\"All ghost variable estimations failed. Check warnings.\")\n",
        "\n",
        "        # Reconstruct DataFrame maintaining original column order for successful estimations\n",
        "        successful_cols = [col for col in target_vars if col in valid_results_map]\n",
        "        ghost_df = pd.concat([valid_results_map[col] for col in successful_cols], axis=1)\n",
        "\n",
        "        failed_cols = set(target_vars) - set(successful_cols)\n",
        "        if failed_cols:\n",
        "             warnings.warn(f\"Ghost variable estimation failed for: {sorted(list(failed_cols))}. Check previous warnings.\", UserWarning)\n",
        "\n",
        "        print(f\"Ghost variable estimation complete. Successfully estimated for {len(successful_cols)} out of {len(target_vars)} variables.\")\n",
        "        return ghost_df #[successful_cols] # Return only successful ones in original relative order"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## %%writefile survival_data_generator.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Type, Dict, Any, List, Union, Optional, Tuple, Callable\n",
        "import warnings\n",
        "\n",
        "try:\n",
        "    from sksurv.util import Surv # Required for scikit-survival format\n",
        "    sksurv_available = True\n",
        "except ImportError:\n",
        "    Surv = None # Handle optional dependency\n",
        "    sksurv_available = False\n",
        "    warnings.warn(\"Package 'scikit-survival' not found. Data generation will not produce sksurv structured arrays.\", ImportWarning)\n",
        "\n",
        "class SurvivalDataGenerator:\n",
        "    \"\"\"\n",
        "    Generates simulated survival data based on specified parameters.\n",
        "    Allows for linear and non-linear predictor effects and censoring.\n",
        "    Inspired by R code in Codes.txt and concepts from Farcomeni & Viviani (2011).\n",
        "\n",
        "    Args:\n",
        "        n_samples (int): Number of samples.\n",
        "        n_features (int): Number of features.\n",
        "        feature_distributions (Optional[Dict[str, Callable]]): Functions to generate each feature.\n",
        "            Defaults to standard normal if None. Function signature should be func(n_samples, rng).\n",
        "        beta_coeffs (Optional[List[float]]): Coefficients for linear predictor exp(beta'x). Use if nonlinear_func is None.\n",
        "        nonlinear_func (Optional[Callable]): Function mapping features X (DataFrame) to non-linear score f(X).\n",
        "        baseline_hazard_lambda (float): Scale parameter for exponential baseline hazard.\n",
        "        censoring_dist_lambda (float): Scale parameter for exponential censoring distribution.\n",
        "                                       Setting close to 0 increases censoring times -> less censoring.\n",
        "        contamination_prop (float): Proportion of observations to contaminate (alpha).\n",
        "        contamination_lambda (float): Parameter for contamination mechanism (original R code logic).\n",
        "                                       *Note: The current implementation uses early/late times, not this lambda.*\n",
        "        random_state (Optional[int]): Seed for reproducibility.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_samples: int = 500,\n",
        "                 n_features: int = 3,\n",
        "                 feature_distributions: Optional[Dict[str, Callable[[int, np.random.Generator], np.ndarray]]] = None,\n",
        "                 beta_coeffs: Optional[List[float]] = None,\n",
        "                 nonlinear_func: Optional[Callable[[pd.DataFrame], np.ndarray]] = None,\n",
        "                 baseline_hazard_lambda: float = 0.01,\n",
        "                 censoring_dist_lambda: float = 0.002, # Lower lambda -> longer censor times -> less censoring\n",
        "                 contamination_prop: float = 0.0,\n",
        "                 contamination_lambda: float = 8.0, # Currently unused in favor of early/late outliers\n",
        "                 random_state: Optional[int] = None):\n",
        "\n",
        "        if beta_coeffs is not None and len(beta_coeffs) != n_features:\n",
        "            raise ValueError(\"Length of beta_coeffs must match n_features.\")\n",
        "        if beta_coeffs is None and nonlinear_func is None:\n",
        "             warnings.warn(\"Neither beta_coeffs nor nonlinear_func provided. Using f(X) = sum(X_i).\", UserWarning)\n",
        "             self.nonlinear_func = lambda x_df: x_df.sum(axis=1).values # Default simple sum\n",
        "             self.beta_coeffs = None\n",
        "        elif beta_coeffs is not None and nonlinear_func is not None:\n",
        "             warnings.warn(\"Both beta_coeffs and nonlinear_func provided. Using nonlinear_func.\", UserWarning)\n",
        "             self.beta_coeffs = None\n",
        "             self.nonlinear_func = nonlinear_func\n",
        "        elif beta_coeffs is not None:\n",
        "             self.beta_coeffs = np.array(beta_coeffs)\n",
        "             self.nonlinear_func = None\n",
        "        else: # nonlinear_func is provided, beta_coeffs is None\n",
        "             self.beta_coeffs = None\n",
        "             self.nonlinear_func = nonlinear_func\n",
        "\n",
        "        self.n_samples = n_samples\n",
        "        self.n_features = n_features\n",
        "        self.feature_names = [f'X{i+1}' for i in range(n_features)]\n",
        "        self.feature_distributions = feature_distributions\n",
        "        self.baseline_hazard_lambda = baseline_hazard_lambda\n",
        "        self.censoring_dist_lambda = censoring_dist_lambda\n",
        "        self.contamination_prop = contamination_prop\n",
        "        self.contamination_lambda = contamination_lambda # Store but maybe don't use directly\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "\n",
        "        print(f\"Initialized SurvivalDataGenerator (n={n_samples}, p={n_features}, alpha={contamination_prop})\")\n",
        "\n",
        "\n",
        "    def _generate_features(self) -> pd.DataFrame:\n",
        "        \"\"\" Generates baseline features based on specified distributions. \"\"\"\n",
        "        features = {}\n",
        "        default_dist = lambda n, rng: rng.normal(0, 1, n)\n",
        "\n",
        "        for i, name in enumerate(self.feature_names):\n",
        "            func = None\n",
        "            if self.feature_distributions:\n",
        "                func = self.feature_distributions.get(name)\n",
        "\n",
        "            if func is None:\n",
        "                if self.feature_distributions is not None: # Only warn if user provided dict but missed a key\n",
        "                     warnings.warn(f\"No distribution function provided for '{name}'. Using default (standard normal).\", UserWarning)\n",
        "                func = default_dist\n",
        "\n",
        "            try:\n",
        "                 # Try calling with rng first\n",
        "                 features[name] = func(self.n_samples, self.rng)\n",
        "            except TypeError:\n",
        "                 try:\n",
        "                      # Fallback for functions not expecting rng\n",
        "                      features[name] = func(self.n_samples)\n",
        "                 except Exception as e:\n",
        "                      raise ValueError(f\"Error calling feature distribution function for '{name}'.\") from e\n",
        "\n",
        "        return pd.DataFrame(features, columns=self.feature_names) # Ensure order\n",
        "\n",
        "\n",
        "    def _calculate_predictor_score(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\" Calculates the linear or non-linear predictor score f(X) or beta'X. \"\"\"\n",
        "        if self.nonlinear_func:\n",
        "            try:\n",
        "                 # Ensure output is 1D array\n",
        "                 score = self.nonlinear_func(X)\n",
        "                 if score.ndim != 1 or score.shape[0] != X.shape[0]:\n",
        "                      raise ValueError(f\"nonlinear_func must return a 1D array of length {X.shape[0]}\")\n",
        "                 return score\n",
        "            except Exception as e:\n",
        "                 raise ValueError(\"Error executing nonlinear_func.\") from e\n",
        "        elif self.beta_coeffs is not None:\n",
        "             if not all(col in X.columns for col in self.feature_names):\n",
        "                  raise ValueError(\"Input DataFrame X is missing expected feature columns for beta calculation.\")\n",
        "             # Ensure we use the columns in the order defined by self.feature_names\n",
        "             return X[self.feature_names].values @ self.beta_coeffs\n",
        "        else:\n",
        "             raise RuntimeError(\"No predictor specified (beta_coeffs or nonlinear_func).\")\n",
        "\n",
        "\n",
        "    def generate(self, return_sksurv_array: bool = True) -> Union[Tuple[pd.DataFrame, np.ndarray], Tuple[pd.DataFrame, pd.Series, pd.Series]]:\n",
        "        \"\"\"\n",
        "        Generates the survival dataset.\n",
        "\n",
        "        Args:\n",
        "            return_sksurv_array (bool): If True, returns y as a scikit-survival structured array.\n",
        "                                        If False, returns separate time and event Series. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            Union[Tuple[pd.DataFrame, np.ndarray], Tuple[pd.DataFrame, pd.Series, pd.Series]]:\n",
        "                - X: DataFrame of features.\n",
        "                - y: Structured array (if return_sksurv_array=True) or Tuple[pd.Series, pd.Series] (time, event)\n",
        "        \"\"\"\n",
        "        if return_sksurv_array and Surv is None:\n",
        "             warnings.warn(\"scikit-survival not found. Returning time and event as separate Series.\", ImportWarning)\n",
        "             return_sksurv_array = False\n",
        "\n",
        "        print(\"Generating survival data...\")\n",
        "        X = self._generate_features()\n",
        "        predictor_score = self._calculate_predictor_score(X)\n",
        "\n",
        "        # Generate latent event times using Exponential PH model\n",
        "        rate = self.baseline_hazard_lambda * np.exp(predictor_score)\n",
        "        rate = np.maximum(rate, 1e-12) # Prevent zero or negative rates\n",
        "        scale = 1.0 / rate\n",
        "        T_latent = self.rng.exponential(scale=scale, size=self.n_samples)\n",
        "\n",
        "        # Contamination\n",
        "        outlier_indices = np.array([], dtype=int)\n",
        "        if self.contamination_prop > 0:\n",
        "            n_outliers = int(np.round(self.contamination_prop * self.n_samples))\n",
        "            if n_outliers > 0 and n_outliers < self.n_samples:\n",
        "                outlier_indices = self.rng.choice(self.n_samples, n_outliers, replace=False)\n",
        "\n",
        "                # Use percentiles for defining early/late ranges relative to generated times\n",
        "                p01 = np.percentile(T_latent, 1)\n",
        "                p99 = np.percentile(T_latent, 99)\n",
        "                max_T = np.max(T_latent) # Max actual latent time\n",
        "\n",
        "                outlier_times_early = self.rng.uniform(0, max(p01 * 0.9, 1e-6), n_outliers) # Ensure upper bound > 0\n",
        "\n",
        "                # **FIXED LOGIC HERE**\n",
        "                late_lower_bound = p99\n",
        "                late_upper_bound = max(p99 * 1.01, max_T * 1.1) # Ensure upper bound is somewhat larger\n",
        "                if late_upper_bound <= late_lower_bound:\n",
        "                    # If p99 and max_T are very close or equal, add a small delta\n",
        "                    late_upper_bound = late_lower_bound + 1e-6 * (late_lower_bound + 1.0) # Relative delta\n",
        "\n",
        "                outlier_times_late = self.rng.uniform(late_lower_bound, late_upper_bound, n_outliers)\n",
        "\n",
        "                u_i = self.rng.binomial(1, 0.5, n_outliers) # Decide if early (1) or late (0) outlier\n",
        "                T_latent[outlier_indices] = np.where(u_i == 1, outlier_times_early, outlier_times_late)\n",
        "                print(f\"Introduced {n_outliers} contaminated time points (early/late).\")\n",
        "\n",
        "            elif n_outliers >= self.n_samples:\n",
        "                 warnings.warn(\"Contamination proportion >= 1, all points considered potentially contaminated.\")\n",
        "\n",
        "\n",
        "        # Generate censoring times\n",
        "        if self.censoring_dist_lambda <= 0:\n",
        "             # If lambda is zero or negative, censoring times are infinite -> no censoring\n",
        "             T_cens = np.full(self.n_samples, np.inf)\n",
        "             print(\"censoring_dist_lambda <= 0 implies effectively no censoring.\")\n",
        "        else:\n",
        "             T_cens = self.rng.exponential(scale=1.0 / self.censoring_dist_lambda, size=self.n_samples)\n",
        "\n",
        "        # Observed time and event status\n",
        "        E_bool = (T_latent <= T_cens)\n",
        "        T_obs = np.minimum(T_latent, T_cens)\n",
        "\n",
        "        # Handle potential NaNs or Infs introduced (e.g. by predictor_score or contamination)\n",
        "        valid_idx = np.isfinite(T_obs) & np.isfinite(predictor_score) & ~np.isnan(T_obs) & ~np.isnan(predictor_score)\n",
        "        if not np.all(valid_idx):\n",
        "             n_removed = self.n_samples - np.sum(valid_idx)\n",
        "             warnings.warn(f\"Removed {n_removed} samples due to non-finite time or predictor score values.\", UserWarning)\n",
        "             X = X[valid_idx].reset_index(drop=True)\n",
        "             T_obs = T_obs[valid_idx]\n",
        "             E_bool = E_bool[valid_idx]\n",
        "             if X.empty:\n",
        "                  raise ValueError(\"All samples removed due to invalid values during generation.\")\n",
        "\n",
        "\n",
        "        actual_censoring_prop = 1.0 - E_bool.mean()\n",
        "        print(f\"Data generation complete. Final n={X.shape[0]}. Actual censoring: {actual_censoring_prop:.2f}\")\n",
        "\n",
        "        if return_sksurv_array:\n",
        "            y_structured = Surv.from_arrays(event=E_bool, time=T_obs)\n",
        "            return X, y_structured\n",
        "        else:\n",
        "            return X, pd.Series(T_obs, name='time'), pd.Series(E_bool, name='event')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sLLS9qQoKJS",
        "outputId": "e8e064b4-f87b-4533-f6fc-ee73e61cc494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting survival_data_generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile nonlinear_survival_model.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from typing import Type, Dict, Any, Optional\n",
        "import warnings\n",
        "\n",
        "# --- Attempt to import scikit-survival ---\n",
        "try:\n",
        "    from sksurv.ensemble import RandomSurvivalForest\n",
        "    from sksurv.util import Surv # Utility for structured array format\n",
        "    # Future: Add other models like GradientBoostingSurvivalAnalysis if needed\n",
        "    sksurv_available = True\n",
        "except ImportError:\n",
        "    # Define dummy classes if sksurv not available, so the rest of the code loads\n",
        "    class RandomSurvivalForest: pass\n",
        "    class Surv: pass\n",
        "    sksurv_available = False\n",
        "    warnings.warn(\"Package 'scikit-survival' not found. NonLinearSurvivalModel class will not function properly.\", ImportWarning)\n",
        "# --- End Import Handling ---\n",
        "\n",
        "class NonLinearSurvivalModel:\n",
        "    \"\"\"\n",
        "    A wrapper class for non-linear survival models from scikit-survival.\n",
        "    Currently implements Random Survival Forest (RSF).\n",
        "\n",
        "    Args:\n",
        "        model_type (str): The type of survival model. Currently only 'rsf' supported.\n",
        "        model_params: Keyword arguments passed to the underlying scikit-survival\n",
        "                      model constructor. Defaults are provided for RSF.\n",
        "    \"\"\"\n",
        "    SUPPORTED_MODELS: Dict[str, Optional[Type[Any]]] = {\n",
        "        'rsf': RandomSurvivalForest if sksurv_available else None,\n",
        "    }\n",
        "\n",
        "    DEFAULT_PARAMS: Dict[str, Dict[str, Any]] = {\n",
        "        'rsf': {'n_estimators': 100, 'random_state': 42, 'n_jobs': -1,\n",
        "                'min_samples_split': 10, 'min_samples_leaf': 5, 'max_depth': None}\n",
        "    }\n",
        "\n",
        "    def __init__(self, model_type: str = 'rsf', **model_params):\n",
        "        if not sksurv_available:\n",
        "            raise ImportError(\"scikit-survival is required for NonLinearSurvivalModel but is not installed.\")\n",
        "\n",
        "        model_key = model_type.lower()\n",
        "        model_class_ref = self.SUPPORTED_MODELS.get(model_key)\n",
        "\n",
        "        if model_class_ref is None:\n",
        "            raise ValueError(f\"Unsupported model_type '{model_type}'. Choose from {list(self.SUPPORTED_MODELS.keys())}.\")\n",
        "\n",
        "        self.model_type: str = model_key\n",
        "        self.model_class: Type[Any] = model_class_ref\n",
        "\n",
        "        # Combine default params with user-provided params\n",
        "        merged_params = self.DEFAULT_PARAMS.get(model_key, {}).copy()\n",
        "        merged_params.update(model_params)\n",
        "        self.model_params: Dict[str, Any] = merged_params\n",
        "\n",
        "        # Instantiate the underlying model\n",
        "        try:\n",
        "            self.model = self.model_class(**self.model_params)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to initialize {self.model_type.upper()} with params {self.model_params}\") from e\n",
        "\n",
        "        self._is_fitted: bool = False\n",
        "        self.feature_names_in_: Optional[list] = None # Store feature names during fit\n",
        "\n",
        "        print(f\"Initialized NonLinearSurvivalModel with {self.model_type.upper()} (params: {self.model_params})\")\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y_structured: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fits the survival model to the training data.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): DataFrame of features for training.\n",
        "            y_structured (np.ndarray): Structured array containing 'event' (bool)\n",
        "                                       and 'time' (float) fields, typically created\n",
        "                                       using sksurv.util.Surv.from_dataframe() or .from_arrays().\n",
        "        \"\"\"\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "             raise ValueError(\"X must be a pandas DataFrame.\")\n",
        "        if not sksurv_available or Surv is None: # Double check just in case\n",
        "             raise ImportError(\"scikit-survival required for fitting.\")\n",
        "        # Basic check for structured array type\n",
        "        if not isinstance(y_structured, np.ndarray) or not all(name in y_structured.dtype.names for name in ['event', 'time']):\n",
        "             raise ValueError(\"y_structured must be a numpy structured array with 'event' and 'time' fields (from sksurv.util.Surv).\")\n",
        "\n",
        "        print(f\"Fitting {self.model_type.upper()}...\")\n",
        "        try:\n",
        "            # Store feature names to ensure consistency during prediction\n",
        "            self.feature_names_in_ = list(X.columns)\n",
        "            # Ensure X contains only numeric data before passing to sksurv model\n",
        "            X_numeric = X.select_dtypes(include=np.number)\n",
        "            if X_numeric.shape[1] < X.shape[1]:\n",
        "                 dropped_cols = list(set(X.columns) - set(X_numeric.columns))\n",
        "                 warnings.warn(f\"Non-numeric columns dropped from X before fitting: {dropped_cols}\", UserWarning)\n",
        "            if X_numeric.isnull().any().any():\n",
        "                 warnings.warn(\"X contains NaNs after numeric selection. Consider imputation. Fitting may fail.\", UserWarning)\n",
        "                 # Basic imputation (median) - replace with more robust strategy if needed\n",
        "                 for col in X_numeric.columns[X_numeric.isnull().any()]:\n",
        "                      median_val = X_numeric[col].median()\n",
        "                      X_numeric[col] = X_numeric[col].fillna(median_val)\n",
        "\n",
        "            if X_numeric.empty:\n",
        "                raise ValueError(\"No numeric features remaining in X after preprocessing.\")\n",
        "\n",
        "            self.model.fit(X_numeric, y_structured)\n",
        "            self._is_fitted = True\n",
        "            print(\"Fitting complete.\")\n",
        "        except Exception as e:\n",
        "            self._is_fitted = False # Ensure state reflects failure\n",
        "            raise RuntimeError(f\"Failed to fit {self.model_type.upper()} model.\") from e\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_risk_score(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predicts the risk score f(x) for new data.\n",
        "        For RSF, higher scores indicate higher risk (closer to log-hazard).\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): DataFrame of features for prediction.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Array of predicted risk scores.\n",
        "        \"\"\"\n",
        "        if not self._is_fitted:\n",
        "            raise NotFittedError(\"This NonLinearSurvivalModel instance is not fitted yet. Call 'fit' first.\")\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "             raise ValueError(\"X must be a pandas DataFrame.\")\n",
        "        if not sksurv_available: # Double check\n",
        "             raise ImportError(\"scikit-survival required for prediction.\")\n",
        "\n",
        "        # Ensure input features match training features order and presence\n",
        "        if self.feature_names_in_ is None:\n",
        "             raise NotFittedError(\"Model has been fitted but feature names were not stored.\")\n",
        "        if list(X.columns) != self.feature_names_in_:\n",
        "             warnings.warn(\"Input features mismatch fitted features order/presence. Reordering/selecting columns.\", UserWarning)\n",
        "             try:\n",
        "                 # Select and reorder columns to match training\n",
        "                 X = X[self.feature_names_in_]\n",
        "             except KeyError as e:\n",
        "                 missing_cols = list(set(self.feature_names_in_) - set(X.columns))\n",
        "                 raise ValueError(f\"Input data missing columns used during fitting: {missing_cols}\") from e\n",
        "\n",
        "        # Ensure X contains only numeric data before passing to sksurv model\n",
        "        X_numeric = X.select_dtypes(include=np.number)\n",
        "        if X_numeric.shape[1] < X.shape[1]:\n",
        "                 dropped_cols = list(set(X.columns) - set(X_numeric.columns))\n",
        "                 warnings.warn(f\"Non-numeric columns dropped from X before prediction: {dropped_cols}\", UserWarning)\n",
        "        if X_numeric.isnull().any().any():\n",
        "             warnings.warn(\"X contains NaNs after numeric selection. Consider imputation. Prediction may fail.\", UserWarning)\n",
        "             # Basic imputation (median)\n",
        "             for col in X_numeric.columns[X_numeric.isnull().any()]:\n",
        "                 median_val = X_numeric[col].median()\n",
        "                 X_numeric[col] = X_numeric[col].fillna(median_val)\n",
        "\n",
        "        if X_numeric.shape[1] != len(self.feature_names_in_):\n",
        "            # Check if only non-numeric columns existed, leading to empty df\n",
        "            if X_numeric.empty and not any(X[f].dtype == np.number for f in self.feature_names_in_):\n",
        "                 raise ValueError(\"Model was fitted on features, but input X has no numeric features corresponding to them.\")\n",
        "            # Or if numeric selection failed for other reasons\n",
        "            raise ValueError(f\"Mismatch in number of numeric features after preprocessing. Expected {len(self.feature_names_in_)}, got {X_numeric.shape[1]}.\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            # predict() for RSF returns risk scores directly\n",
        "            risk_scores = self.model.predict(X_numeric)\n",
        "            return risk_scores\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to predict with {self.model_type.upper()} model.\") from e\n",
        "\n",
        "    @property\n",
        "    def is_fitted(self) -> bool:\n",
        "        return self._is_fitted\n",
        "\n",
        "# --- Example Usage (if run directly) ---\n",
        "if __name__ == '__main__':\n",
        "    # Assuming GhostVariableEstimator and SurvivalDataGenerator classes are defined above or imported\n",
        "\n",
        "    if not sksurv_available:\n",
        "        print(\"scikit-survival not installed. Skipping NonLinearSurvivalModel example.\")\n",
        "    else:\n",
        "        print(\"\\n--- Testing NonLinearSurvivalModel ---\")\n",
        "        # Generate some non-linear data using the corrected generator\n",
        "        try:\n",
        "            generator = SurvivalDataGenerator(\n",
        "                n_samples=300, n_features=3,\n",
        "                nonlinear_func=lambda df: 0.5 * df['X1'] * df['X2'] + np.sin(df['X3'] * np.pi),\n",
        "                censoring_dist_lambda=0.02,\n",
        "                random_state=789\n",
        "            )\n",
        "            X_train, y_train_sksurv = generator.generate(return_sksurv_array=True)\n",
        "            X_test, y_test_sksurv = generator.generate(return_sksurv_array=True) # Generate separate test set\n",
        "\n",
        "            # Initialize and fit the RSF model\n",
        "            nl_model = NonLinearSurvivalModel(model_type='rsf', n_estimators=50, min_samples_leaf=10, random_state=789)\n",
        "            nl_model.fit(X_train, y_train_sksurv)\n",
        "\n",
        "            # Predict risk scores on test data\n",
        "            if nl_model.is_fitted:\n",
        "                risk_scores_test = nl_model.predict_risk_score(X_test)\n",
        "                print(\"\\nPredicted Risk Scores (f(x)) on Test Set (Head):\")\n",
        "                print(risk_scores_test[:5])\n",
        "\n",
        "                # Check model performance (optional - using concordance index)\n",
        "                try:\n",
        "                    from sksurv.metrics import concordance_index_censored\n",
        "                    # Ensure y_test_sksurv has boolean 'event' field\n",
        "                    event_indicator = y_test_sksurv['event'].astype(bool)\n",
        "                    time_indicator = y_test_sksurv['time']\n",
        "                    c_index = concordance_index_censored(event_indicator, time_indicator, risk_scores_test)\n",
        "                    print(f\"\\nTest Set Concordance Index: {c_index[0]:.4f}\")\n",
        "                except ImportError:\n",
        "                    print(\"\\nCannot calculate Concordance Index - sksurv.metrics not found?\")\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError calculating Concordance Index: {e}\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Skipping example because scikit-survival is not installed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the example execution: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPHDvmMq07vz",
        "outputId": "ce3e6f68-7bcd-4303-970d-3fb6fc403734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing NonLinearSurvivalModel ---\n",
            "Initialized SurvivalDataGenerator (n=300, p=3, alpha=0.0)\n",
            "Generating survival data...\n",
            "Data generation complete. Final n=300. Actual censoring: 0.64\n",
            "Generating survival data...\n",
            "Data generation complete. Final n=300. Actual censoring: 0.66\n",
            "Initialized NonLinearSurvivalModel with RSF (params: {'n_estimators': 50, 'random_state': 789, 'n_jobs': -1, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_depth': None})\n",
            "Fitting RSF...\n",
            "Fitting complete.\n",
            "\n",
            "Predicted Risk Scores (f(x)) on Test Set (Head):\n",
            "[28.82265679 34.52978731 27.1673239  27.15886642 39.82709137]\n",
            "\n",
            "Test Set Concordance Index: 0.6703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile ghost_cox_interpreter.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from typing import Type, Dict, Any, Optional, Tuple\n",
        "import warnings\n",
        "\n",
        "# Assuming the following classes are defined in separate files or above\n",
        "# from nonlinear_survival_model import NonLinearSurvivalModel\n",
        "# from ghost_variables import GhostVariableEstimator # Assuming corrected version\n",
        "\n",
        "class GhostCoxInterpreter:\n",
        "    \"\"\"\n",
        "    Interprets a fitted non-linear survival model using the Ghost Variables methodology.\n",
        "\n",
        "    Calculates variable relevance (RV_gh) based on the change in the model's\n",
        "    predicted risk score when a variable is replaced by its ghost.\n",
        "    Optionally calculates the Relevance Matrix V.\n",
        "\n",
        "    Args:\n",
        "        survival_model (NonLinearSurvivalModel): A fitted instance of the survival model wrapper.\n",
        "        ghost_estimator (GhostVariableEstimator): An initialized instance for estimating ghost variables.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 survival_model: 'NonLinearSurvivalModel',\n",
        "                 ghost_estimator: 'GhostVariableEstimator'):\n",
        "\n",
        "        # (Constructor code remains the same as before)\n",
        "        if not hasattr(survival_model, 'predict_risk_score') or not callable(survival_model.predict_risk_score):\n",
        "             raise TypeError(\"survival_model must have a callable 'predict_risk_score' method.\")\n",
        "        if not hasattr(survival_model, 'is_fitted') or not survival_model.is_fitted:\n",
        "             raise NotFittedError(\"The provided survival_model must be fitted first.\")\n",
        "        if not isinstance(ghost_estimator, GhostVariableEstimator):\n",
        "             raise TypeError(\"ghost_estimator must be an instance of GhostVariableEstimator.\")\n",
        "\n",
        "        self.survival_model = survival_model\n",
        "        self.ghost_estimator = ghost_estimator\n",
        "        print(\"Initialized GhostCoxInterpreter.\")\n",
        "\n",
        "\n",
        "    def calculate_relevance(self, X_test: pd.DataFrame, calculate_relevance_matrix: bool = True) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
        "        \"\"\"\n",
        "        Calculates ghost variable relevance RV_gh for each variable in X_test.\n",
        "\n",
        "        Args:\n",
        "            X_test (pd.DataFrame): The test dataset (features only).\n",
        "            calculate_relevance_matrix (bool): If True, also calculates and returns the\n",
        "                                               Relevance Matrix V. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
        "                - relevance_df: DataFrame containing relevance scores (currently RV_gh numerator)\n",
        "                                and ranks for each variable.\n",
        "                - V_df: DataFrame representing the Relevance Matrix V (or None if\n",
        "                        calculate_relevance_matrix is False).\n",
        "        \"\"\"\n",
        "        if not isinstance(X_test, pd.DataFrame) or X_test.empty:\n",
        "            raise ValueError(\"X_test must be a non-empty pandas DataFrame.\")\n",
        "        if not self.survival_model.is_fitted:\n",
        "             raise NotFittedError(\"The survival model within the interpreter is not fitted.\")\n",
        "\n",
        "        print(\"\\nCalculating Ghost Variable Relevance...\")\n",
        "        n_samples, n_features_orig = X_test.shape\n",
        "        feature_names_orig = list(X_test.columns)\n",
        "\n",
        "        # 1. Estimate all ghost variables for the test set\n",
        "        X_ghost_test = self.ghost_estimator.estimate_all_ghosts(X_test)\n",
        "\n",
        "        # Determine successfully ghosted features\n",
        "        valid_feature_names = list(X_ghost_test.columns)\n",
        "        if not valid_feature_names:\n",
        "             raise RuntimeError(\"Ghost estimation failed for all variables.\")\n",
        "        if len(valid_feature_names) < n_features_orig:\n",
        "             missing_ghosts = set(feature_names_orig) - set(valid_feature_names)\n",
        "             warnings.warn(f\"Could not estimate ghosts for: {missing_ghosts}. Excluding these from relevance calculation.\", UserWarning)\n",
        "\n",
        "        # Use only the features for which ghosts were successfully estimated\n",
        "        X_test_filtered = X_test[valid_feature_names]\n",
        "        n_samples, n_features = X_test_filtered.shape # Update dimensions\n",
        "\n",
        "        # 2. Get predictions from the survival model on original (filtered) test data\n",
        "        try:\n",
        "            # Ensure model prediction uses the same filtered feature set if necessary\n",
        "            f_X_test = self.survival_model.predict_risk_score(X_test_filtered)\n",
        "            if f_X_test.ndim != 1 or len(f_X_test) != n_samples:\n",
        "                 raise ValueError(f\"predict_risk_score returned unexpected shape {f_X_test.shape}\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Failed to get predictions from the survival model on filtered X_test.\") from e\n",
        "\n",
        "        relevance_results = {}\n",
        "        A_matrix = np.zeros((n_samples, n_features)) # Size based on valid features\n",
        "\n",
        "        # 3. Loop through each VALID variable to calculate relevance\n",
        "        for j, var_name in enumerate(valid_feature_names):\n",
        "            print(f\"... calculating relevance for {var_name} ({j+1}/{n_features})\")\n",
        "            X_test_j_ghost = X_test_filtered.copy()\n",
        "            try:\n",
        "                # Use .loc for safer assignment and handle potential type issues\n",
        "                X_test_j_ghost.loc[:, var_name] = X_ghost_test[var_name].astype(X_test_filtered[var_name].dtype, errors='ignore')\n",
        "            except Exception:\n",
        "                 X_test_j_ghost.loc[:, var_name] = X_ghost_test[var_name] # Assign directly if casting fails\n",
        "\n",
        "            # Get predictions with the ghost variable substituted\n",
        "            try:\n",
        "                f_X_test_j_ghost = self.survival_model.predict_risk_score(X_test_j_ghost)\n",
        "                if f_X_test_j_ghost.shape != f_X_test.shape:\n",
        "                     raise ValueError(f\"Prediction shape mismatch for ghost '{var_name}': {f_X_test_j_ghost.shape} vs {f_X_test.shape}\")\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Could not get prediction for ghost '{var_name}': {e}. Setting relevance to NaN.\", UserWarning)\n",
        "                relevance_results[var_name] = {'RV_gh_numerator': np.nan}\n",
        "                A_matrix[:, j] = np.nan # Mark column as invalid\n",
        "                continue\n",
        "\n",
        "            # Calculate change and store\n",
        "            prediction_change = f_X_test - f_X_test_j_ghost\n",
        "            A_matrix[:, j] = prediction_change\n",
        "            mean_sq_change = np.mean(prediction_change**2)\n",
        "            relevance_results[var_name] = {'RV_gh_numerator': mean_sq_change}\n",
        "\n",
        "        # --- *** CORRECTED LINE HERE *** ---\n",
        "        # Create DataFrame with feature names as index\n",
        "        relevance_df = pd.DataFrame.from_dict(relevance_results, orient='index')\n",
        "        # --- *** END CORRECTION *** ---\n",
        "\n",
        "        # Filter out any NaNs that might have occurred during prediction step\n",
        "        nan_features = relevance_df[relevance_df['RV_gh_numerator'].isna()].index.tolist()\n",
        "        if nan_features:\n",
        "             warnings.warn(f\"Relevance calculation resulted in NaN for: {nan_features}. Excluding them.\", UserWarning)\n",
        "             relevance_df = relevance_df.dropna(subset=['RV_gh_numerator'])\n",
        "             # Also filter corresponding columns from A_matrix if calculating V\n",
        "             valid_indices_for_V = [j for j, name in enumerate(valid_feature_names) if name not in nan_features]\n",
        "             A_matrix = A_matrix[:, valid_indices_for_V]\n",
        "             valid_feature_names_for_V = relevance_df.index.tolist() # Update list for V\n",
        "        else:\n",
        "            A_matrix_valid = A_matrix # Use the full A_matrix if no NaNs\n",
        "            valid_feature_names_for_V = valid_feature_names\n",
        "\n",
        "\n",
        "        if relevance_df.empty:\n",
        "             raise RuntimeError(\"Relevance calculation failed or resulted in NaN for all variables.\")\n",
        "\n",
        "        relevance_df['Rank'] = relevance_df['RV_gh_numerator'].rank(ascending=False).astype(int)\n",
        "\n",
        "        # 4. Calculate Relevance Matrix V (optional)\n",
        "        V_df = None\n",
        "        if calculate_relevance_matrix:\n",
        "             if not valid_feature_names_for_V: # Check if any valid features remain for V\n",
        "                  warnings.warn(\"No valid features remaining to calculate Relevance Matrix V.\", UserWarning)\n",
        "             else:\n",
        "                  print(\"... calculating Relevance Matrix V\")\n",
        "                  # Use only valid columns/rows from A_matrix\n",
        "                  V_matrix = (1.0 / n_samples) * (A_matrix_valid.T @ A_matrix_valid)\n",
        "                  V_df = pd.DataFrame(V_matrix, index=valid_feature_names_for_V, columns=valid_feature_names_for_V)\n",
        "\n",
        "        print(\"Relevance calculation complete.\")\n",
        "        return relevance_df.sort_values('Rank'), V_df\n",
        "\n",
        "# --- Example Usage (requires all previous classes) ---\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Assuming GhostVariableEstimator, SurvivalDataGenerator, NonLinearSurvivalModel classes are defined\n",
        "    # AND scikit-survival, pygam, joblib are installed\n",
        "\n",
        "    print(\"\\n--- Testing GhostCoxInterpreter ---\")\n",
        "    try:\n",
        "        # 1. Generate Data\n",
        "        generator = SurvivalDataGenerator(\n",
        "            n_samples=400, n_features=4, # Add a less important feature X4\n",
        "            nonlinear_func=lambda df: 0.6 * df['X1'] + np.sin(df['X2'] * np.pi) - 0.1*df['X3']**2 + 0.01*df['X4'], # X4 is weak\n",
        "            censoring_dist_lambda=0.015,\n",
        "            random_state=111\n",
        "        )\n",
        "        # Ensure classes are available (replace imports if needed)\n",
        "        # from survival_data_generator import SurvivalDataGenerator\n",
        "        # from ghost_variables import GhostVariableEstimator\n",
        "        # from nonlinear_survival_model import NonLinearSurvivalModel\n",
        "\n",
        "        X_train, y_train_sksurv = generator.generate(return_sksurv_array=True)\n",
        "        X_test, y_test_sksurv = generator.generate(return_sksurv_array=True)\n",
        "\n",
        "        # 2. Fit Survival Model\n",
        "        nl_model = NonLinearSurvivalModel(model_type='rsf', n_estimators=100, min_samples_leaf=15, random_state=111)\n",
        "        nl_model.fit(X_train, y_train_sksurv)\n",
        "\n",
        "        # 3. Initialize Ghost Estimator\n",
        "        # Using n_jobs=1 (serial) to avoid potential pickling errors seen before\n",
        "        ghost_estimator_rf = GhostVariableEstimator(estimator_type='rf', n_jobs=1, random_state=111)\n",
        "\n",
        "        # 4. Initialize and Run Interpreter\n",
        "        interpreter = GhostCoxInterpreter(survival_model=nl_model, ghost_estimator=ghost_estimator_rf)\n",
        "        relevance_df, V_matrix_df = interpreter.calculate_relevance(X_test, calculate_relevance_matrix=True)\n",
        "\n",
        "        print(\"\\n--- Relevance Results (Unnormalized) ---\")\n",
        "        print(relevance_df)\n",
        "\n",
        "        if V_matrix_df is not None:\n",
        "            print(\"\\n--- Relevance Matrix V (Unnormalized) ---\")\n",
        "            print(V_matrix_df.round(4))\n",
        "\n",
        "            # Example: Eigen analysis of V\n",
        "            try:\n",
        "                eigenvalues, eigenvectors = np.linalg.eigh(V_matrix_df)\n",
        "                idx = eigenvalues.argsort()[::-1]\n",
        "                eigenvalues = eigenvalues[idx]\n",
        "                eigenvectors = eigenvectors[:,idx]\n",
        "                print(\"\\n--- Eigenvalues of V (Descending) ---\")\n",
        "                print(np.round(eigenvalues, 5))\n",
        "                print(\"\\n--- First Eigenvector Components ---\")\n",
        "                print(pd.Series(eigenvectors[:, 0], index=V_matrix_df.index).round(4))\n",
        "            except np.linalg.LinAlgError:\n",
        "                print(\"\\nCould not perform eigen decomposition on V matrix.\")\n",
        "\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"\\nSkipping example because a required package is missing: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during the GhostCoxInterpreter example: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaX-w5kY1kZe",
        "outputId": "13cf4ebe-fc53-4cba-c61b-41f493625b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing GhostCoxInterpreter ---\n",
            "Initialized SurvivalDataGenerator (n=400, p=4, alpha=0.0)\n",
            "Generating survival data...\n",
            "Data generation complete. Final n=400. Actual censoring: 0.58\n",
            "Generating survival data...\n",
            "Data generation complete. Final n=400. Actual censoring: 0.62\n",
            "Initialized NonLinearSurvivalModel with RSF (params: {'n_estimators': 100, 'random_state': 111, 'n_jobs': -1, 'min_samples_split': 10, 'min_samples_leaf': 15, 'max_depth': None})\n",
            "Fitting RSF...\n",
            "Fitting complete.\n",
            "Initialized GhostVariableEstimator with RF (params: {'n_estimators': 100, 'random_state': 111, 'n_jobs': 1, 'max_depth': 10, 'min_samples_leaf': 5}, requested n_jobs=1)\n",
            "Initialized GhostCoxInterpreter.\n",
            "\n",
            "Calculating Ghost Variable Relevance...\n",
            "\n",
            "Estimating all ghost variables using RF...\n",
            "Ghost variable estimation complete. Successfully estimated for 4 out of 4 variables.\n",
            "... calculating relevance for X1 (1/4)\n",
            "... calculating relevance for X2 (2/4)\n",
            "... calculating relevance for X3 (3/4)\n",
            "... calculating relevance for X4 (4/4)\n",
            "... calculating Relevance Matrix V\n",
            "Relevance calculation complete.\n",
            "\n",
            "--- Relevance Results (Unnormalized) ---\n",
            "    RV_gh_numerator  Rank\n",
            "X2      1159.828371     1\n",
            "X1       519.211647     2\n",
            "X4        43.054866     3\n",
            "X3        34.294954     4\n",
            "\n",
            "--- Relevance Matrix V (Unnormalized) ---\n",
            "          X1         X2       X3       X4\n",
            "X1  519.2116    18.9301 -19.4022 -14.6946\n",
            "X2   18.9301  1159.8284   5.9649 -28.7485\n",
            "X3  -19.4022     5.9649  34.2950   1.2244\n",
            "X4  -14.6946   -28.7485   1.2244  43.0549\n",
            "\n",
            "--- Eigenvalues of V (Descending) ---\n",
            "[1161.17383  519.84548   41.96696   33.40357]\n",
            "\n",
            "--- First Eigenvector Components ---\n",
            "X1   -0.0299\n",
            "X2   -0.9992\n",
            "X3   -0.0047\n",
            "X4    0.0261\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy scipy pandas scikit-learn scikit-survival pygam joblib matplotlib seaborn\n",
        "!pip install --no-cache-dir numpy scipy pandas scikit-learn scikit-survival pygam joblib matplotlib seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sBIP9OdpFD4y",
        "outputId": "9b79b82d-04ec-4798-ba7a-127c102d3182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy-1.26.4.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: scipy 1.11.4\n",
            "Uninstalling scipy-1.11.4:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy-1.11.4.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled scipy-1.11.4\n",
            "Found existing installation: pandas 2.2.3\n",
            "Uninstalling pandas-2.2.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/pandas-2.2.3.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/pandas/*\n",
            "Proceed (Y/n)? T\n",
            "Your response ('t') was not one of the expected responses: y, n, \n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled pandas-2.2.3\n",
            "Found existing installation: scikit-learn 1.6.1\n",
            "Uninstalling scikit-learn-1.6.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/scikit_learn-1.6.1.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/sklearn/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled scikit-learn-1.6.1\n",
            "Found existing installation: scikit-survival 0.24.1\n",
            "Uninstalling scikit-survival-0.24.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/scikit_survival-0.24.1.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/sksurv/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled scikit-survival-0.24.1\n",
            "Found existing installation: pygam 0.9.1\n",
            "Uninstalling pygam-0.9.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/pygam-0.9.1.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/pygam/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled pygam-0.9.1\n",
            "Found existing installation: joblib 1.4.2\n",
            "Uninstalling joblib-1.4.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/joblib-1.4.2.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/joblib/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled joblib-1.4.2\n",
            "Found existing installation: matplotlib 3.10.1\n",
            "Uninstalling matplotlib-3.10.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/matplotlib-3.10.1.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/matplotlib/*\n",
            "    /usr/local/lib/python3.11/dist-packages/mpl_toolkits/axes_grid1/*\n",
            "    /usr/local/lib/python3.11/dist-packages/mpl_toolkits/axisartist/*\n",
            "    /usr/local/lib/python3.11/dist-packages/mpl_toolkits/mplot3d/*\n",
            "    /usr/local/lib/python3.11/dist-packages/pylab.py\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled matplotlib-3.10.1\n",
            "y\n",
            "Found existing installation: seaborn 0.13.2\n",
            "Uninstalling seaborn-0.13.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/seaborn-0.13.2.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/seaborn/*\n",
            "Proceed (Y/n)?   Successfully uninstalled seaborn-0.13.2\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting scikit-survival\n",
            "  Downloading scikit_survival-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m215.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygam\n",
            "  Downloading pygam-0.9.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting joblib\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: ecos in /usr/local/lib/python3.11/dist-packages (from scikit-survival) (2.0.14)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from scikit-survival) (2.10.2)\n",
            "Requirement already satisfied: osqp<1.0.0,>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from scikit-survival) (0.6.7.post3)\n",
            "Requirement already satisfied: progressbar2<5.0.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pygam) (4.5.0)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m251.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/dist-packages (from osqp<1.0.0,>=0.6.3->scikit-survival) (0.1.7.post5)\n",
            "Requirement already satisfied: python-utils>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from progressbar2<5.0.0,>=4.2.0->pygam) (3.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: typing_extensions>3.10.0.2 in /usr/local/lib/python3.11/dist-packages (from python-utils>=3.8.1->progressbar2<5.0.0,>=4.2.0->pygam) (4.13.1)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m269.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m262.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_survival-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m262.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygam-0.9.1-py3-none-any.whl (522 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.0/522.0 kB\u001b[0m \u001b[31m229.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m196.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m266.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m283.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m171.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m283.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, joblib, scipy, pandas, scikit-learn, pygam, matplotlib, seaborn, scikit-survival\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.4.2 matplotlib-3.10.1 numpy-1.26.4 pandas-2.2.3 pygam-0.9.1 scikit-learn-1.6.1 scikit-survival-0.24.1 scipy-1.11.4 seaborn-0.13.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "sksurv"
                ]
              },
              "id": "61fdd8c61cc14015aaa6ef21d9736cbf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile experiment_runner.py # Use if in a notebook\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "from typing import List, Dict, Any, Optional, Callable, Tuple\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split # Simple train/test split\n",
        "import inspect # To check constructor args\n",
        "\n",
        "# Assuming these classes are defined in other files or above\n",
        "# from survival_data_generator import SurvivalDataGenerator\n",
        "# from ghost_variables import GhostVariableEstimator\n",
        "# from nonlinear_survival_model import NonLinearSurvivalModel\n",
        "# from ghost_cox_interpreter import GhostCoxInterpreter\n",
        "\n",
        "try:\n",
        "    from sksurv.metrics import concordance_index_censored\n",
        "    from sksurv.util import Surv\n",
        "    sksurv_available = True\n",
        "except ImportError:\n",
        "    concordance_index_censored = None\n",
        "    Surv = None\n",
        "    sksurv_available = False\n",
        "    warnings.warn(\"scikit-survival not found. C-index calculation and plotting might fail.\", ImportWarning)\n",
        "\n",
        "class ExperimentRunner:\n",
        "    \"\"\"\n",
        "    Runs simulation experiments to evaluate the GhostCox interpretability method.\n",
        "\n",
        "    Handles data generation, model fitting, ghost estimation, relevance calculation,\n",
        "    performance evaluation (C-index), and basic result aggregation/plotting.\n",
        "\n",
        "    Args:\n",
        "        scenarios (List[Dict[str, Any]]): A list of scenario configurations.\n",
        "                                          Each dict should define keys like 'name',\n",
        "                                          'data_params', 'model_params', 'ghost_params'.\n",
        "        n_replicates (int): Number of times to run each scenario. Defaults to 10.\n",
        "        test_size (float): Proportion of data to use for the test set. Defaults to 0.3.\n",
        "        random_state_base (Optional[int]): Base seed for reproducibility. Each replicate\n",
        "                                           will use random_state_base + replicate_index.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 scenarios: List[Dict[str, Any]],\n",
        "                 n_replicates: int = 10,\n",
        "                 test_size: float = 0.3,\n",
        "                 random_state_base: Optional[int] = 42):\n",
        "\n",
        "        if not sksurv_available:\n",
        "            raise ImportError(\"scikit-survival is required for ExperimentRunner.\")\n",
        "\n",
        "        self.scenarios = scenarios\n",
        "        self.n_replicates = n_replicates\n",
        "        self.test_size = test_size\n",
        "        self.random_state_base = random_state_base\n",
        "        self.results = [] # List to store results from each run\n",
        "        self._summary_df = None # Cache for summary results\n",
        "        print(f\"Initialized ExperimentRunner with {len(scenarios)} scenarios and {n_replicates} replicates.\")\n",
        "\n",
        "    def _run_single_replication(self, scenario_config: Dict[str, Any], replicate_id: int) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\" Executes one full replication for a given scenario. \"\"\"\n",
        "        scenario_name = scenario_config.get('name', f'Scenario_{replicate_id}')\n",
        "        run_seed = None\n",
        "        # --- *** CORRECTED SYNTAX HERE *** ---\n",
        "        if self.random_state_base is not None:\n",
        "             # Ensure different seed for each replicate/scenario combo\n",
        "             # Use a formula that depends on both scenario index and replicate id\n",
        "             scenario_index = self.scenarios.index(scenario_config) if scenario_config in self.scenarios else 0\n",
        "             run_seed = self.random_state_base + (scenario_index * self.n_replicates) + replicate_id\n",
        "        # --- *** END CORRECTION *** ---\n",
        "\n",
        "        print(f\"\\n--- Running: {scenario_name}, Replicate: {replicate_id+1}/{self.n_replicates} (Seed: {run_seed}) ---\")\n",
        "        start_time = time()\n",
        "\n",
        "        # --- Make sure class definitions are available ---\n",
        "        # Dynamically check if they exist in the global scope or raise error\n",
        "        required_classes = ['SurvivalDataGenerator', 'NonLinearSurvivalModel', 'GhostVariableEstimator', 'GhostCoxInterpreter']\n",
        "        for cls_name in required_classes:\n",
        "            if cls_name not in globals() and cls_name not in locals():\n",
        "                 # A simple check; might need more robust import logic depending on file structure\n",
        "                 raise NameError(f\"Class '{cls_name}' is not defined. Ensure it's imported or defined before running.\")\n",
        "\n",
        "        try:\n",
        "            # 1. Generate Data\n",
        "            data_params = scenario_config.get('data_params', {}).copy()\n",
        "            # Pass run_seed to generator if it accepts random_state\n",
        "            if 'random_state' in inspect.signature(SurvivalDataGenerator.__init__).parameters:\n",
        "                 data_params['random_state'] = run_seed\n",
        "            generator = SurvivalDataGenerator(**data_params)\n",
        "            X, y_sksurv = generator.generate(return_sksurv_array=True)\n",
        "\n",
        "            min_events_required = 5\n",
        "            if y_sksurv['event'].sum() < min_events_required:\n",
        "                 warnings.warn(f\"[{scenario_name} Rep {replicate_id+1}] Insufficient events ({y_sksurv['event'].sum()}) generated. Skipping replication.\", UserWarning)\n",
        "                 return None\n",
        "\n",
        "            # 2. Split Data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y_sksurv, test_size=self.test_size, random_state=run_seed, stratify=y_sksurv['event'] if y_sksurv['event'].sum() > 1 else None # Stratify if possible\n",
        "            )\n",
        "            if X_train.empty or X_test.empty or y_test['event'].sum() < 1: # Check test set events\n",
        "                 warnings.warn(f\"[{scenario_name} Rep {replicate_id+1}] Train/test split resulted in empty data or no events in test set. Skipping.\", UserWarning)\n",
        "                 return None\n",
        "\n",
        "            # 3. Fit Survival Model\n",
        "            model_params = scenario_config.get('model_params', {}).copy()\n",
        "            if 'random_state' in inspect.signature(NonLinearSurvivalModel.__init__).parameters:\n",
        "                 model_params['random_state'] = run_seed\n",
        "            survival_model = NonLinearSurvivalModel(**model_params)\n",
        "            survival_model.fit(X_train, y_train)\n",
        "\n",
        "            # 4. Calculate C-Index on Test Set\n",
        "            risk_scores_test = survival_model.predict_risk_score(X_test)\n",
        "            c_index_result = concordance_index_censored(\n",
        "                y_test['event'].astype(bool), y_test['time'], risk_scores_test\n",
        "            )\n",
        "            c_index = c_index_result[0] if c_index_result else np.nan\n",
        "\n",
        "            # 5. Initialize Ghost Estimator\n",
        "            ghost_params = scenario_config.get('ghost_params', {}).copy()\n",
        "            if 'random_state' in inspect.signature(GhostVariableEstimator.__init__).parameters:\n",
        "                  ghost_params['random_state'] = run_seed\n",
        "            ghost_estimator = GhostVariableEstimator(**ghost_params)\n",
        "\n",
        "            # 6. Run Interpreter\n",
        "            interpreter_params = scenario_config.get('interpreter_params', {}).copy()\n",
        "            interpreter = GhostCoxInterpreter(survival_model=survival_model, ghost_estimator=ghost_estimator)\n",
        "            relevance_df, V_matrix_df = interpreter.calculate_relevance(\n",
        "                X_test,\n",
        "                calculate_relevance_matrix=interpreter_params.get('calculate_relevance_matrix', True)\n",
        "            )\n",
        "\n",
        "            # 7. Store results\n",
        "            run_duration = time() - start_time\n",
        "            result = {\n",
        "                'scenario_name': scenario_name,\n",
        "                'replicate_id': replicate_id,\n",
        "                'random_seed': run_seed,\n",
        "                'c_index': c_index,\n",
        "                'relevance_df': relevance_df,\n",
        "                'V_matrix_df': V_matrix_df,\n",
        "                'duration_sec': run_duration,\n",
        "                'n_features': X.shape[1],\n",
        "                'n_train': X_train.shape[0],\n",
        "                'n_test': X_test.shape[0],\n",
        "                'ghost_estimator_type': ghost_estimator.estimator_key,\n",
        "                'survival_model_type': survival_model.model_type,\n",
        "                'error': None # Indicate success\n",
        "            }\n",
        "            print(f\"--- Completed: {scenario_name}, Replicate: {replicate_id+1} (C-Index: {c_index:.4f}, Time: {run_duration:.2f}s) ---\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"--- FAILED: {scenario_name}, Replicate: {replicate_id+1} ---\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            # Attempt to retrieve params even on failure\n",
        "            ghost_est_type = scenario_config.get('ghost_params', {}).get('estimator_type', 'N/A')\n",
        "            surv_model_type = scenario_config.get('model_params', {}).get('model_type', 'N/A')\n",
        "            return {\n",
        "                'scenario_name': scenario_name,\n",
        "                'replicate_id': replicate_id,\n",
        "                'random_seed': run_seed,\n",
        "                'error': f\"{type(e).__name__}: {e}\", # More informative error\n",
        "                'duration_sec': time() - start_time,\n",
        "                'c_index': np.nan,\n",
        "                'relevance_df': None,\n",
        "                'V_matrix_df': None,\n",
        "                 'ghost_estimator_type': ghost_est_type,\n",
        "                 'survival_model_type': surv_model_type,\n",
        "            }\n",
        "\n",
        "\n",
        "    def run_experiment(self):\n",
        "        \"\"\" Runs all replicates for all defined scenarios. \"\"\"\n",
        "        print(f\"\\n===== Starting Experiment ({len(self.scenarios)} scenarios, {self.n_replicates} replicates each) =====\")\n",
        "        self.results = [] # Clear previous results\n",
        "        self._summary_df = None # Clear cached summary\n",
        "        total_start_time = time()\n",
        "\n",
        "        for i, scenario in enumerate(self.scenarios):\n",
        "             print(f\"\\n===== Running Scenario {i+1}/{len(self.scenarios)}: {scenario.get('name', 'Unnamed')} =====\")\n",
        "             for rep in range(self.n_replicates):\n",
        "                  result = self._run_single_replication(scenario, rep)\n",
        "                  if result:\n",
        "                       self.results.append(result)\n",
        "\n",
        "        total_duration = time() - total_start_time\n",
        "        print(f\"\\n===== Experiment Finished (Total Time: {total_duration:.2f}s) =====\")\n",
        "        # Generate and store summary DataFrame\n",
        "        self._summary_df = self._generate_results_summary()\n",
        "        return self._summary_df\n",
        "\n",
        "\n",
        "    def get_results_summary(self) -> Optional[pd.DataFrame]:\n",
        "         \"\"\" Returns the generated summary DataFrame (or generates it if needed). \"\"\"\n",
        "         if hasattr(self, '_summary_df') and self._summary_df is not None:\n",
        "             return self._summary_df\n",
        "         elif self.results:\n",
        "             self._summary_df = self._generate_results_summary()\n",
        "             return self._summary_df\n",
        "         else:\n",
        "             print(\"No results collected yet. Run experiment first.\")\n",
        "             return None\n",
        "\n",
        "\n",
        "    def _generate_results_summary(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\" Creates a summary DataFrame from the collected results (long format). \"\"\"\n",
        "        if not self.results:\n",
        "            return None\n",
        "\n",
        "        summary_data = []\n",
        "        # Get a set of all variables encountered across successful runs\n",
        "        all_vars = set()\n",
        "        for res in self.results:\n",
        "            if res.get('relevance_df') is not None and isinstance(res['relevance_df'], pd.DataFrame):\n",
        "                 all_vars.update(res['relevance_df'].index)\n",
        "        all_vars_list = sorted(list(all_vars))\n",
        "\n",
        "        for res in self.results:\n",
        "            base_info = {\n",
        "                'scenario_name': res['scenario_name'],\n",
        "                'replicate_id': res['replicate_id'],\n",
        "                'c_index': res.get('c_index', np.nan),\n",
        "                'duration_sec': res['duration_sec'],\n",
        "                'ghost_estimator': res.get('ghost_estimator_type', 'N/A'),\n",
        "                'survival_model': res.get('survival_model_type', 'N/A'),\n",
        "                'error': res.get('error', None)\n",
        "            }\n",
        "            if res.get('relevance_df') is not None and isinstance(res['relevance_df'], pd.DataFrame):\n",
        "                relevance = res['relevance_df']['RV_gh_numerator']\n",
        "                ranks = res['relevance_df']['Rank']\n",
        "                # Ensure row for each possible variable, filling with NaN if not present in this run\n",
        "                for var_name in all_vars_list:\n",
        "                    row = base_info.copy()\n",
        "                    row.update({\n",
        "                        'variable': var_name,\n",
        "                        'relevance_score': relevance.get(var_name, np.nan), # Use .get\n",
        "                        'relevance_rank': ranks.get(var_name, np.nan),\n",
        "                    })\n",
        "                    summary_data.append(row)\n",
        "            else: # Handle error case\n",
        "                 row = base_info.copy()\n",
        "                 row.update({\n",
        "                     'variable': 'N/A',\n",
        "                     'relevance_score': np.nan,\n",
        "                     'relevance_rank': np.nan,\n",
        "                 })\n",
        "                 summary_data.append(row)\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        return summary_df\n",
        "\n",
        "    # --- Plotting Methods (remain the same as before) ---\n",
        "    def plot_c_index_comparison(self, summary_df: Optional[pd.DataFrame] = None):\n",
        "        \"\"\" Plots boxplots of C-index values across scenarios. \"\"\"\n",
        "        if summary_df is None:\n",
        "            summary_df = self.get_results_summary()\n",
        "            if summary_df is None or summary_df.empty:\n",
        "                 print(\"No summary data available for plotting C-index.\")\n",
        "                 return\n",
        "\n",
        "        c_index_df = summary_df[['scenario_name', 'replicate_id', 'c_index']].dropna().drop_duplicates()\n",
        "        if c_index_df.empty:\n",
        "             print(\"No valid C-index data found for plotting.\")\n",
        "             return\n",
        "\n",
        "        n_scenarios = len(c_index_df['scenario_name'].unique())\n",
        "        plt.figure(figsize=(max(6, n_scenarios * 1.2), 5))\n",
        "        sns.boxplot(data=c_index_df, x='scenario_name', y='c_index', palette='viridis')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.title('Concordance Index Comparison Across Scenarios')\n",
        "        plt.ylabel('C-Index')\n",
        "        plt.xlabel('Scenario')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_relevance_comparison(self, variable: str, summary_df: Optional[pd.DataFrame] = None):\n",
        "        \"\"\" Plots boxplots of relevance scores for a specific variable across scenarios. \"\"\"\n",
        "        if summary_df is None:\n",
        "            summary_df = self.get_results_summary()\n",
        "            if summary_df is None or summary_df.empty:\n",
        "                 print(\"No summary data available for plotting relevance.\")\n",
        "                 return\n",
        "\n",
        "        var_df = summary_df[(summary_df['variable'] == variable) & summary_df['relevance_score'].notna()]\n",
        "        if var_df.empty:\n",
        "             print(f\"No valid results found for variable '{variable}'.\")\n",
        "             return\n",
        "\n",
        "        n_scenarios = len(var_df['scenario_name'].unique())\n",
        "        plt.figure(figsize=(max(6, n_scenarios * 1.2), 5))\n",
        "        sns.boxplot(data=var_df, x='scenario_name', y='relevance_score', palette='viridis')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.title(f'Relevance Score (RV_gh Numerator) for Variable: {variable}')\n",
        "        plt.ylabel('Relevance Score')\n",
        "        plt.xlabel('Scenario')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_rank_comparison(self, variable: str, summary_df: Optional[pd.DataFrame] = None, rank_limit: Optional[int]=None):\n",
        "        \"\"\" Plots distribution of ranks for a specific variable across scenarios. \"\"\"\n",
        "        if summary_df is None:\n",
        "            summary_df = self.get_results_summary()\n",
        "            if summary_df is None or summary_df.empty:\n",
        "                 print(\"No summary data available for plotting ranks.\")\n",
        "                 return\n",
        "\n",
        "        var_df = summary_df[(summary_df['variable'] == variable) & summary_df['relevance_rank'].notna()]\n",
        "        if var_df.empty:\n",
        "             print(f\"No valid rank results found for variable '{variable}'.\")\n",
        "             return\n",
        "\n",
        "        n_scenarios = len(var_df['scenario_name'].unique())\n",
        "        plt.figure(figsize=(max(6, n_scenarios * 1.2), 5))\n",
        "        sns.stripplot(data=var_df, x='scenario_name', y='relevance_rank', jitter=0.2, alpha=0.6, palette='viridis', s=5)\n",
        "        sns.pointplot(data=var_df, x='scenario_name', y='relevance_rank', estimator=np.median, errorbar=None, color='black', markers='D', linestyles='', zorder=10, dodge=True)\n",
        "\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.title(f'Relevance Rank Distribution for Variable: {variable}')\n",
        "        plt.ylabel('Rank (1 = Most Relevant)')\n",
        "        plt.xlabel('Scenario')\n",
        "\n",
        "        # Determine rank limits for y-axis\n",
        "        all_ranks = summary_df.loc[summary_df['relevance_rank'].notna(), 'relevance_rank']\n",
        "        if not all_ranks.empty:\n",
        "            max_rank_overall = all_ranks.max()\n",
        "            min_rank_overall = 1 # Ranks start at 1\n",
        "            if rank_limit is None:\n",
        "                 rank_limit = int(max_rank_overall)\n",
        "            # Set y-axis ticks to integers\n",
        "            plt.yticks(np.arange(min_rank_overall, rank_limit + 1, step=max(1, rank_limit // 10))) # Adjust step for clarity\n",
        "            plt.ylim(bottom=min_rank_overall - 0.5, top=rank_limit + 0.5)\n",
        "        else:\n",
        "            plt.ylim(bottom=0.5, top = 1.5) # Default if no ranks\n",
        "\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# --- Example Usage (ensure prerequisite classes are defined/imported) ---\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    if not sksurv_available:\n",
        "         print(\"Cannot run ExperimentRunner example: scikit-survival not installed.\")\n",
        "    else:\n",
        "        print(\"\\n--- Testing ExperimentRunner ---\")\n",
        "        try:\n",
        "             # If classes are in separate files, uncomment imports:\n",
        "             # from survival_data_generator import SurvivalDataGenerator\n",
        "             # from ghost_variables import GhostVariableEstimator\n",
        "             # from nonlinear_survival_model import NonLinearSurvivalModel\n",
        "             # from ghost_cox_interpreter import GhostCoxInterpreter\n",
        "\n",
        "             # 1. Define Scenarios\n",
        "             def nl_func_example(df):\n",
        "                  return 0.8 * df['X1'] + np.cos(df['X2'] * np.pi) - 0.1 * df['X3']**2 + 0.0 * df['X4'] # X4 is noise\n",
        "\n",
        "             f_dist = {f'X{i+1}': (lambda n, rng: rng.normal(0, 1, n)) for i in range(4)}\n",
        "\n",
        "             scenarios_def = [\n",
        "                 {\n",
        "                     'name': 'NL_RSF_RFGhost',\n",
        "                     'data_params': {'n_samples': 300, 'n_features': 4, 'nonlinear_func': nl_func_example,\n",
        "                                     'feature_distributions': f_dist, 'censoring_dist_lambda': 0.01, 'contamination_prop': 0.0},\n",
        "                     'model_params': {'model_type': 'rsf', 'n_estimators': 50, 'min_samples_leaf': 10},\n",
        "                     'ghost_params': {'estimator_type': 'rf', 'n_jobs': 1}\n",
        "                 },\n",
        "                 {\n",
        "                     'name': 'NL_RSF_LMGhost',\n",
        "                     'data_params': {'n_samples': 300, 'n_features': 4, 'nonlinear_func': nl_func_example,\n",
        "                                     'feature_distributions': f_dist, 'censoring_dist_lambda': 0.01, 'contamination_prop': 0.0},\n",
        "                     'model_params': {'model_type': 'rsf', 'n_estimators': 50, 'min_samples_leaf': 10},\n",
        "                     'ghost_params': {'estimator_type': 'lm', 'n_jobs': 1}\n",
        "                 },\n",
        "                  {\n",
        "                     'name': 'NL_RSF_RFGhost_Contam',\n",
        "                     'data_params': {'n_samples': 300, 'n_features': 4, 'nonlinear_func': nl_func_example,\n",
        "                                     'feature_distributions': f_dist, 'censoring_dist_lambda': 0.01, 'contamination_prop': 0.1},\n",
        "                     'model_params': {'model_type': 'rsf', 'n_estimators': 50, 'min_samples_leaf': 10},\n",
        "                     'ghost_params': {'estimator_type': 'rf', 'n_jobs': 1}\n",
        "                 },\n",
        "                 # Add GAM ghost scenario if pygam is installed\n",
        "                 {'name': 'NL_RSF_GAMGhost',\n",
        "                  'data_params': {'n_samples': 300, 'n_features': 4, 'nonlinear_func': nl_func_example,\n",
        "                                  'feature_distributions': f_dist, 'censoring_dist_lambda': 0.01, 'contamination_prop': 0.0},\n",
        "                  'model_params': {'model_type': 'rsf', 'n_estimators': 50, 'min_samples_leaf': 10},\n",
        "                  'ghost_params': {'estimator_type': 'gam', 'n_jobs': 1}\n",
        "                  } if GhostVariableEstimator.SUPPORTED_ESTIMATORS.get('gam') else None\n",
        "             ]\n",
        "             # Filter out None scenarios (e.g., GAM if pygam not installed)\n",
        "             scenarios_def = [s for s in scenarios_def if s is not None]\n",
        "\n",
        "\n",
        "             # 2. Initialize and Run Experiment Runner\n",
        "             runner = ExperimentRunner(scenarios=scenarios_def, n_replicates=5, random_state_base=2024)\n",
        "             summary = runner.run_experiment()\n",
        "\n",
        "             # 3. Analyze Results\n",
        "             if summary is not None and not summary.empty:\n",
        "                  print(\"\\n--- Experiment Summary (Failed Runs) ---\")\n",
        "                  print(summary[summary['error'].notna()])\n",
        "\n",
        "                  print(\"\\n--- Experiment Summary (Averages for Successful Runs) ---\")\n",
        "                  success_summary = summary[summary['error'].isna()]\n",
        "                  print(success_summary.groupby('scenario_name')[['c_index', 'duration_sec']].mean().round(4))\n",
        "\n",
        "                  print(\"\\n--- Average Rank per Variable per Scenario (Successful Runs) ---\")\n",
        "                  avg_ranks = success_summary.pivot_table(index='scenario_name', columns='variable', values='relevance_rank', aggfunc='mean')\n",
        "                  print(avg_ranks.round(2))\n",
        "\n",
        "                  # 4. Plot Results\n",
        "                  runner.plot_c_index_comparison()\n",
        "                  runner.plot_relevance_comparison('X2') # Important variable\n",
        "                  runner.plot_rank_comparison('X2')     # Important variable\n",
        "                  runner.plot_rank_comparison('X4')     # Noise variable\n",
        "\n",
        "        except ImportError as e:\n",
        "             print(f\"\\nSkipping ExperimentRunner example due to missing package: {e}\")\n",
        "        except NameError as e:\n",
        "              print(f\"\\nSkipping ExperimentRunner example because a required class definition is missing: {e}. Ensure all class codes are executed.\")\n",
        "        except Exception as e:\n",
        "             print(f\"\\nAn error occurred during the ExperimentRunner example: {e}\")\n",
        "             import traceback\n",
        "             traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "wGgau-raEKGQ",
        "outputId": "99085917-56ab-4cad-f20c-41c0ddf58348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-6a0b94312718>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m \u001b[0;31m# Simple train/test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m \u001b[0;31m# To check constructor args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmiscplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maxisgrid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_no_scipy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'vq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hierarchy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_testutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/cluster/vq.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_vq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0m__docformat__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'restructuredtext'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_vq.pyx\u001b[0m in \u001b[0;36minit scipy.cluster._vq\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    }
  ]
}